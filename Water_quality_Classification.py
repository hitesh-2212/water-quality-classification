# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MdcVN54WUU29Dktz5BabYDBdSgmQsjbJ

## Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import f_oneway
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
import warnings
warnings.filterwarnings('ignore')

"""## Dataset-Overview"""

df=pd.read_csv('water_quality.csv', encoding='latin1')
df.head().T

# Shape of dataset
df.shape

# Columns
df.columns

# Dataset Info
df.info()

# Duplicate record Count
df.duplicated().sum()

# Nan Value Counts
df.isna().sum()

# Unique Value Count in Each Column
df.nunique()

# Numeric Features statistical Measure
df.describe().T

"""**Dataset Structure-->**

The dataset contains 222 rows and 54 columns, representing water quality observations collected across multiple monitoring stations.

There are no duplicate records, indicating each observation is uniquely captured.

Data includes a mix of metadata, environmental observations, and physico-chemical parameters.


**Data Types & Consistency-->**

The dataset is highly heterogeneous, with 40 object-type columns, 13 numerical (float), and 1 integer column.

Several quantitative water quality parameters (e.g., Dissolved Oxygen, Nitrate N, Coliform counts) are stored as object types, suggesting the presence of non-numeric values or formatting issues.

This inconsistency indicates the dataset is not model-ready in its raw form.


**Numerical Distribution & Scale Issues-->**

Key water quality parameters show extreme variation and skewness:

Conductivity ranges from 45 to 74,240, with a mean far higher than the median.

BOD and COD exhibit heavy right-skew, with maximum values far exceeding typical safe limits.

Total Dissolved Solids (TDS) and Hardness show very large ranges, indicating samples from both low and highly mineralized waters.

Parameters such as pH and Temperature are relatively stable but still include out-of-bound values, suggesting potential water quality risks.

**Presence of Outliers-->**

Several numerical variables have extreme maximum values:

BOD up to 210, COD up to 624, Hardness up to 7390, TDS up to 38,400.

These values suggest either severely polluted water bodies or measurement/data entry anomalies, requiring further validation.

The large gap between mean and median across many features confirms non-normal distributions.

## Data Cleaning
"""

# Standardize Column NAmes -->
df.columns=(df.columns.str.strip().str.lower().str.replace(' ','_'))
df.columns

# Droping Empty Columns:-->
df.drop(columns=['use_of_water_in_down_stream'],inplace=True)

# Droping rows with missing Target:-->
df=df[df['use_based_class'].notna()]    # 7 Rows Missing

# Dropping Highly Missing Columns Or Low Importance Cols:-->
cols_to_drop=cols_to_drop = ['odor','remark','major_polluting_sources','visibility_effluent_discharge','human_activities','floating_matter','color']

df.drop(columns=cols_to_drop, inplace=True)

# Convert Numeric-Like Object Columns To Numeric Dta Type:-->

# Identifying All Object Cols:-

object_cols=df.select_dtypes(include='object').columns
print(object_cols)

# Extracting the Columns with Numeric Values:-
numeric_object_cols = ['dissolved_o2','nitrate_n','fecal_coliform','total_coliform','fecal_streptococci','turbidity','phenophelene_alkanity',
    'total_alkalinity','chlorides','total_kjeldahl_n','sulphate','sodium','total_suspended_solids','phosphate','boron','potassium','flouride']
# some of these object columns contains str+numeric values in format like 5(BDL)so removing the string portion here:-->

for col in numeric_object_cols:
    df[col] = (df[col].astype(str).str.replace('BDL', '', regex=False)   # remove all BDL
        .str.replace('(', '', regex=False)
        .str.replace(')', '', regex=False)
        .replace(['ND', 'nd', '--', ''], np.nan)
        .astype(float))
                                                #nan is also mis-placed with ND/nd/--/'' in some places.

df[numeric_object_cols].info()    # These Object columns are now successfully Converted to Numeric Columns.(Float)

# Conerting Date Column INto Date Time D-Type:-->
df['sampling_date'] = pd.to_datetime(df['sampling_date'], errors='coerce')

# Dropping Identifier Columns Anf Location Based Columns:-
id_cols = ['stn_code','stn_name','state_name','mon_agency','frequency','latitude', 'longitude','name_of_water_body', 'river_basin'] # No Value as Feature in Model...

df.drop(columns=id_cols, inplace=True)

# Columns With Nan Values:-->
nan_cols=df.columns[df.isna().sum()>0]
print(f'Columns With Null Values:- \n \n {list(nan_cols)}')

# Checking Skewness Of These Numeric Features:-
skewness=df[nan_cols].skew()
print(f'\nSkewNess of Numeric Columns:-\n \n {skewness.sort_values(ascending=False)}')

# All the Columns are Skewed:-
# REplacing the nan with Median Column Values:-
for col in nan_cols:
  df[col]=df[col].fillna(df[col].median())

# Final Check-->
print(df.isna().sum())

# Checking Target Variable..
print(f'Distribution of Target..\n{df['use_based_class'].value_counts(normalize=True)}')
# The Target Variable Contains 'Not InFormation' Category..
df=df[df['use_based_class']!='No Information']

# Target Column Contains Details about the Classes(Creating niw Col..)
df['water_quality_class'] = df['use_based_class'].str.extract(r'^([A-E])')

# Droping old Column..
df.drop(columns=['use_based_class'], inplace=True)

# Final Class Distribution
print(f'Final Target Class Distribution..\n {df['water_quality_class'].value_counts(normalize=True)}')

# Final Shape Of cleaned dataset
df.shape

"""## EDA :-
**Understanding the Variables..**
"""

# Target Variable Distribution:-->
df['water_quality_class'].value_counts(normalize=True)

# Visualization of Target Feature..
plt.figure(figsize=(8,5))
sns.countplot(x='water_quality_class', data=df)
plt.title("Water Quality Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# Statistical discription of numeric data..
df.describe().T

# Dissolved_O2 vs Water_quality_class:-

plt.figure(figsize=(6,4))
sns.boxplot(x='water_quality_class', y='dissolved_o2', data=df)
plt.title("Dissolved Oxygen vs Water Quality Class")
plt.show()          #Class C has lower DO than A, B, and E --> polluted class behavior
                    #Low DO outliers (near 0–1) appear in A and E, indicating:  seasonal or local effects

# BOD V/s Water Quality..

plt.figure(figsize=(6,4))
sns.boxplot(x='water_quality_class', y='bod', data=df)
plt.title("BOD vs Water Quality Class")
plt.show()    #Class A shows huge outliers
              #Very large BOD values (50, 100, 200+) :-Episodic pollution OR Industrial discharge events
              #Class E surprisingly has low median BOD

# Total Coliform V/S Water Quality..

plt.figure(figsize=(6,4))
sns.boxplot(x='water_quality_class', y='total_coliform', data=df)
plt.title("Total Coliform vs Water Quality Class")          #Class B has the highest coliform levels
plt.show()

# Distribution of Conductivity..

plt.figure(figsize=(6,4))
sns.histplot(df['conductivity'], kde=True)           #Presence of extreme mineral pollution
plt.title("Distribution of Conductivity")         #Extreme right skew
plt.show()

# Ph V/s Water Quality..

plt.figure(figsize=(6,4))
sns.boxplot(x='water_quality_class', y='ph', data=df) #A few high-pH outliers in Class A (>9)
plt.title("pH vs Water Quality Class")       #Most values lie between 7.4 and 8.5

plt.show()

# Correlation Heatmap Of numeric Features..
plt.figure(figsize=(12,8))
corr = df.select_dtypes(include='number').corr()
sns.heatmap(corr, cmap='coolwarm', center=0)
plt.title("Correlation Heatmap of Numeric Features")
plt.show()

"""### Key Findings from Exploratory Data Analysis (EDA)
1:-Strong Class Imbalance:-

Water quality classes are highly imbalanced, with Class A (~82%) dominating the dataset.

2:-Use-Based Classes Are Not Defined by a Single Parameter

No individual water quality parameter (DO, BOD, pH, etc.) shows a clean, monotonic separation across classes.


3️:-Dissolved Oxygen Shows Overlap Across Classes

Although DO is a key regulatory parameter, its distribution overlaps heavily across classes.

DO alone is insufficient as a standalone discriminator.

4️:-BOD Highlights Human Activity Rather Than Overall Quality

Class B (outdoor bathing) exhibits the highest median BOD, likely due to concentrated human activity.

Class A shows extreme BOD outliers, indicating episodic pollution even in intended drinking water sources.


5️:-Total Coliform Is More Informative but Still Overlapping

Total coliform levels show clearer differentiation, especially with higher values for Class B.


6️:-pH Remains Within a Narrow, Controlled Range

pH values are largely constrained between 7.4 and 8.5 across all classes.

Heavy overlap indicates that pH is well-regulated and not a strong class differentiator.


7️:-Several Features Exhibit Extreme Right Skew

Parameters such as conductivity, TDS, COD, chlorides, and coliform counts show heavy right-skewed distributions.

Extreme values represent real pollution events, not data errors.


**Conclusion:-**

EDA Confirms the Need for Multivariate, Class-Weighted Models

Class overlap, skewed distributions, and feature interactions suggest that:

Simple rule-based or threshold-based classification would fail.

Multivariate machine learning models are appropriate for this problem.
"""

# Class-wise median table.
grouped_medians = (
    df.groupby('water_quality_class').median(numeric_only=True))

grouped_medians.T     #each feature behaviour for each water quality class.

"""Class B is NOT “between” A and C

It has extreme mineral / ionic pollution

Class E is biologically polluted

Class C is moderate/mixed

## Feature Selection
"""

target_col='water_quality_class'

# Numerical features.
num_features = df.select_dtypes(include=["int64", "float64"]).columns.tolist()

anova_results = []

for feature in num_features:
    groups = []

    for cls in df[target_col].unique():
        group = df[df[target_col] == cls][feature]
        if len(group) > 1:
            groups.append(group)

    if len(groups) > 1:
        f_stat, p_val = f_oneway(*groups)
        anova_results.append({
            "Feature": feature,
            "F-Statistic": f_stat,
            "p-value": p_val
        })

anova_df = pd.DataFrame(anova_results).sort_values("p-value")
anova_df

# Defining significance based on p_value..
significant_num_features = anova_df[anova_df["p-value"] < 0.05]["Feature"].tolist()
significant_num_features

key_features = ['ph','dissolved_o2','bod','cod','total_coliform','fecal_coliform','nitrate_n','turbidity','temperature','conductivity']

final_candidates=significant_num_features+key_features
final_features=list(set(final_candidates))
final_features

# After Removing Redundant Features:-
final_features=['ph','temperature','dissolved_o2','cod','fecal_coliform','nitrate_n','turbidity','conductivity','hardness_caco3','total_alkalinity']

# Lebel Encoded the Target Feature..

le = LabelEncoder()
df['target_encoded'] = le.fit_transform(df['water_quality_class'])

x = df[final_features]
y = df['target_encoded']

# Scale the features.
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

model = LogisticRegression(
    multi_class='multinomial',
    solver='lbfgs',
    max_iter=1000
)

model.fit(x_scaled, y)

# Multicolinierity Between Final Features:-
vif_df = pd.DataFrame()
vif_df["feature"] = final_features
vif_df["VIF"] = [variance_inflation_factor(x_scaled, i)
                 for i in range(x_scaled.shape[1])]
vif_df

"""vif_score < 5 for all features: Good to go"""

x = x_scaled
y = df['target_encoded']

# Splits
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=42)

"""## Model-1:-Logistic Regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    multi_class='multinomial',
    solver='lbfgs',
    max_iter=1000,
    class_weight='balanced'
)
model.fit(x_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(x_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""Severe class imbalance affects results

Minority classes (B, C) have only 1 sample each in test data

Their high recall is statistically unreliable

Major confusion between Class A and Class C

13 Class A samples misclassified as Class C

Shows strong feature overlap between these classes
"""

from sklearn.model_selection import StratifiedKFold, cross_val_score
pipeline = Pipeline([
('scaler', StandardScaler()),
('clf', model)
])


cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


scores = cross_val_score(
pipeline,
x, y,
cv=cv,
scoring='f1_macro'
)


print("F1 Macro scores:", scores)
print("Mean F1 Macro:", scores.mean())

"""Baseline objective achieved

Model served its purpose as a diagnostic baseline

Demonstrated need for non-linear models

## Model-2:-Random-Forest
"""

from sklearn.ensemble import RandomForestClassifier


rf = RandomForestClassifier(
n_estimators=300,
class_weight='balanced',
random_state=42
)


rf.fit(x_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix

y_pred = rf.predict(x_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(
    n_splits=5,
    shuffle=True,
    random_state=42
)

rf_cv_scores = cross_val_score(
    rf,
    x,
    y,
    cv=cv,
    scoring='f1_macro'
)

print("RF F1 Macro scores:", rf_cv_scores)
print("Mean RF F1 Macro:", rf_cv_scores.mean())

rf_final = RandomForestClassifier(
    n_estimators=300,
    class_weight='balanced',
    random_state=42
)

rf_final.fit(x, y)

feature_importance = pd.DataFrame({
'feature': final_features,
'importance': rf_final.feature_importances_
}).sort_values(by='importance', ascending=False)


feature_importance

"""No single feature dominates — water quality is driven by multiple interacting factors"""

pipeline = Pipeline([
    ('smote', SMOTE(random_state=42,k_neighbors=1)),
    ('rf', RandomForestClassifier(
        n_estimators=300,
        random_state=42
    ))
])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(
    pipeline,
    x, y,
    cv=cv,
    scoring='f1_macro'
)

scores.mean()

"""SMOTE improved the macro F1-score, but I didn’t use it because the minority classes were too small to generate meaningful synthetic samples.

## Conclusion

This project successfully developed a robust water quality classification system by combining domain knowledge with statistical and machine learning techniques. After thorough exploratory analysis and feature selection using ANOVA and multicollinearity checks, a balanced and interpretable set of water quality parameters was identified. Multiple models were evaluated, and while multinomial logistic regression served as a strong baseline, a class-weighted Random Forest model performed better by capturing non-linear relationships among water quality indicators.

Model performance was evaluated using stratified cross-validation and macro F1-score to address severe class imbalance. Although oversampling methods such as SMOTE improved scores, they were not adopted due to the extremely small size of minority classes and the risk of generating unrealistic synthetic samples. Feature importance and class-wise median analysis confirmed that mineral composition, chemical pollution, and biological contamination jointly influence water quality. Overall, the final model provides a reliable and interpretable approach to water quality classification under real-world data constraints.
"""